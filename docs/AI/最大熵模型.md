# 最大熵模型   
   

> https://transwarpio.github.io/teaching_ml/2017/08/15/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/    

## 1. 什么是最大熵原理
- 例子1:假设随机变量X有5个取值{A,B,C,D,E},要估计各个值的概率$P(A),P(B),…,P(E)$.
- 这些概率值满足条件$P(A)+P(B)+P(C)+P(D)+P(E)=1$
- 但是满足这个条件的概率分布有无数个。如果没有其他信息,一个可行的办法就是认为他们的概率都相等,均为0.2。
- 如果再加一个条件$P(A) + P(B) = 0.3$,那么各个值的概率为多少?    

## 2. 数学知识
### 拉格朗日乘子法     
问题：求函数 $z=f(x)$在条件$\phi_{i}(x)=0 (i=1,2,...,m)$下的可能极值点，其中 $x=(x_1,x_2,...,x_n) \in R^n$ .   
利用Lagrange乘子法，可将上述带约束的极值问题转化为无约束极值问题来进行求解，具体求解步骤如下：
1.构造函数$L(x)=f(x)+ \sum_{i=1}^{m} \lambda_i \phi_i(x)$,其中 $\lambda_i(i=1,2,...,m)$为Lagrange乘子.    
2.求解方程组    
$$ 
\left\{
\begin{array}{c}   
\frac{\partial L}{\partial x} &= 0   \\
\phi_i(x) &= 0     &\text{,   i=1,2,...,m} \tag{1.1}  
\end{array}
\right.
$$     
其中$\frac{\partial L}{\partial x}=(\frac{\partial L}{\partial x_1},\frac{\partial L}{\partial x_2},\frac{\partial L}{\partial x},...,\frac{\partial L}{\partial x_n})^T$表示L关于x的梯度。    


### Bayes定理
- Bayes定理用来描述两个条件概率之间的关系。若计P(A)和P(B)分别表示事件A和事件B发生的概率,P(A|B)表示事件B发生的情况下事件A发生的概率,P(A,B)表示事件A和B同时发生的概率,则有:       
$$
P(A|B)=\frac{P(A,B)}{P(B)}   \tag{1.2}
$$

$$
P(B|A)=\frac{P(A,B)}{P(A)}   \tag{1.3}  
$$

利用(1.2)和(1.3)可以进一步得到贝叶斯公式:      
$$
P(A|B)=P(A)\frac{P(B|A)}{P(B)}  \tag{1.4}
$$    

### 熵
- 熵(entropy)是热力学中的概念,由香浓引入到信息论中。在信息论和概率统计中,熵用来表示随机变量不确定性的度量。      
**定义**设$X \in \{x_1,x_2,...,x_n\}$为一个离散随机变量，其概率分布为$p(X=x_i)=p_i,i=1,2,...,n$,则$X$的熵为   
$$
H(X)=- \sum_{i=1}^{n}p_i log  p_i \tag{1.5} 
$$


$H(x)$依赖于$X$的分布,而与$X$的具体值无关。$H(X)$越大,表示$X$的不确定性越大。     

### 条件熵    
**定义** 设$X \in \{x_1,x_2,...,x_n\},Y \in \{y_1,y_2,...,y_m\}$为离散随机变量，在已知$X$的条件下，$Y$的条件熵$(conditional    entropy)$可定义为     
$$
H(Y|X)=\sum_{i=1}^{n}p(x_i)H(Y|X=x_i)=-\sum_{i=1}^{n}p(x_i) \sum_{j=1}^{m}p(y_j|x_i)logp(y_j|x_i)   \tag{1.8}
$$     
它表示已知$X$的条件下，$Y$的条件概率分布的熵对$X$的数学期望。   



## 3.最大熵模型的定义
- 最大熵原理是统计学习的一般原理,将它应用到分类就得到了最大熵模型
- 假设分类模型是一个条件概率分布P(Y|X),X表示输入,Y表示输出。这个模型表示的是对于给定的输入X,以条件概率P(Y|X)输出Y。
- 给定一个训练数据集T,我们的目标就是利用最大熵原理选择最好的分类模型。       
$$
\tau=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}  \tag{3.1}
$$ 

- 按照最大熵原理,我们应该优先保证模型满足已知的所有约束。那么如何得到这些约束呢?
- 思路是:从训练数据T中抽取若干特征,然后要求这些特征在T上关于经验分布的期望与它们在模型中关于p(x,y)的数学期望相等,这样,一个特征就对应一个约束。    


### 特征函数    
**例** 假设我们需要判断"打"字是动词还是量词，已知的训练数据有    
$$
\begin{aligned}
(x_1,y_1) &= (一打火柴，量词)   \\
(x_2,y_2) &= (三打啤酒，量词)   \\
(x_3,y_3) &= (五打塑料袋，量词)  \\
(x_4,y_4) &= (打电话，动词)     \\
(x_5,y_5) &= (打篮球，动词)     \\
...
\end{aligned}
$$
通过观察，我们发现，"打"字前面为数字时，"打"是量词，"打"后面为名词时，"打"是动词，这就是从训练数据中提取的两个特征，可分别用特征函数表示为    
$$
f_1(x,y)= 
        \begin{cases}
        1,  & \text{若"打"字前面为数字；} \\
        0, & \text{否则.}
        \end{cases}   
        \\
f_2(x,y)= 
        \begin{cases}
        1,  & \text{若"打"字后面为名词；} \\
        0, & \text{否则.}
        \end{cases}
$$    
定义了这两个特征函数后，对于训练数据，我们便有   
$f_1(x_1,y_1)=f_1(x_2,y_2)=f_1(x_3,y_3)=1;f_1(x_4,y_4)=f_1(x_5,y_5)=0;...$    
$f_2(x_1,y_1)=f_2(x_2,y_2)=f_2(x_3,y_3)=0;f_2(x_4,y_4)=f_2(x_5,y_5)=2;...$


### 经验分布
- 经验分布是指通过训练数据T上进行统计得到的分布。我们需要考察两个经验分布,分别是x,y的联合经验分布以及x的分布。其定义如下:     
$$
\hat{p}(x,y)=\frac{count(x,y)}{N},\hat{p}(x)=\frac{count(x)}{N}, \tag{3.3}
$$    

- (3.3)中count(x,y)表示(x,y)在数据T中出现的次数,count(x)表示x在数据T中出现的次数。    

### 约束条件
- 对于任意的特征函数f,记$E_{\hat{p}}(f)$ 表示f在训练数据T上关于$\hat{p}(x,y)$ 的数学期望。 $E_{p}(f)$表示f在模型上关于p(x,y)的数学期望。按照期望的定义,有:     
$$
E_{\hat{p}}(f) = \sum_{x,y}\hat{p}(x,y)f(x,y),   \tag{3.4}    
$$
$$
E_{p}(f) = \sum_{x,y}p(x,y)f(x,y),    \tag{3.5} 
$$

- 我们需要注意的是公式(3.5)中的p(x,y)是未知的。并且我们建模的
目标是p(y|x),因此我们利用Bayes定理得到p(x,y)=p(x)p(y|x)。
此时,p(x)也还是未知,我们可以使用经验分布对p(x)进行近似。     
$$
E_{p}(f)=\sum_{x,y}\hat{p}(x)p(y|x)f(x,y).   \tag{3.6}
$$     

- 对于概率分布p(y|x),我们希望特征f的期望应该和从训练数据中得到的特征期望是一样的。因此,可以提出约束:   
$$
E_p(f) = E_{\hat{p}}(f).   \tag{3.7} 
$$
$$
\sum_{x,y} \hat{p}(x)p(y|x)f(x,y) = \sum \hat{p}(x,y)f(x,y).  \tag{3.8}  
$$


- 假设从训练数据中抽取了n个特征,相应的便有n个特征函数以及n个约束条件。    
$${C_i}:E_{p}(f_i)=
E_{\hat{p}}(f_i):=
\tau_{i}
,i=1,2,...,n  \tag{3.9}$$

### 最大熵模型
- 给定数据集T,我们的目标就是根据最大熵原理选择一个最优的分类器。
- 已知特征函数和约束条件,我们将熵的概念应用到条件分布上面去。我们采用条件熵。     
$$
H(p(y|x))=-\sum_{x,y}\hat{p}(x)p(y|x)logp(y|x).   \tag{3.12}
$$  

- 至此,我们可以给出最大熵模型的完整描述了。对于给定的数据集T,特征函数f i (x,y),i=1,…,n,最大熵模型就是求解模型集合C中条件熵最大的模型:     
$$
\min_{p \in C} -H(p) = (\sum_{x,y}\hat{p}(x)p(y|x)logp(y|x)),  \tag{3.12}  
$$
$$
s.t. \sum_{x,y}\hat{p}(x)p(y|x)f_i(x,y) = \tau_i   \\
\sum_{y}p(y|x) = 1   \tag{3.13}
$$  

## 4.最大熵模型的学习
- 最大熵模型的学习过程就是求解最大熵模型的过程。求解约束最优化问题(3.12),(3.13)所得的解就是最大熵模型学习的解。思路如下:
- 利用拉格朗日乘子法将最大熵模型由一个带约束的最优化问题转化为一个与之等价的无约束的最优化问题,它是一个min max问题。
- 利用对偶问题的等价性,将原始问题转换为一个max min问题。

### 原始问题和对偶问题
- 利用拉格朗日乘子法定义关于(3.7)、(3.12)和(3.13)的拉格朗日函数如下:    
$$
\begin{aligned}
L(p,\lambda) &= -H(p) + \lambda_0(1- \sum_{y}p(y|x)) + \sum_{i=1}^{n}\lambda_i(\tau_i - E_{p}(f_i))  \\
&= \sum_{x,y}\hat{p}(x)p(y|x)logp(y|x) + \lambda_0(1- \sum_{y}p(y|x)) + \sum_{i=1}^{n}\lambda_i(\tau_i - \sum_{x,y}\hat{p}(x)p(y|x)f_i(x,y))   \tag{4.1}
\end{aligned}
$$

- 利用拉格朗日对偶性,(3.6)、(3.12)和(3.13)定义的最大熵模型等价于求解:    
$$

$$  
通过交换极大和极小的位置,可以得到公式(4.2)的对偶问题:       
经过两次等价转换,求解最大熵模型,就是求解对偶问题(4.3)就可以了。
极小问题求解
对偶问题(4.3)内部的极小问题是关于参数lamba的问题    

我们可以利用拉格朗日乘子法获取p。
首先计算拉格朗日函数L对p(y|x)的偏导数。    


