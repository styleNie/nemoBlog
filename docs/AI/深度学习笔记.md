<!-- ---
title: "深度学习笔记"    
author:     
date: March 16, 2018    

toc:    
  depth_from: 1    
  depth_to: 6    
  ordered: false    
  ignoreLink:false     
  
html:    
  embed_local_images: true     
  embed_svg: true     
  offline: false    
  toc: false    

print_background: false    

export_on_save:    
  html: true    

---
-->
# <center>深度学习笔记</center>    

## 1. 神经元模型   
&emsp;&emsp;历史上，科学家一直希望模拟人的大脑，造出可以思考的机器。人为什么能思考？生物学家们通过实验发现，人之所以能思考，在于人的大脑中复杂的神经网络，而神经网络是由一个个神经元组成的。1904年，西班牙科学家SR Cajal在意大利解剖学家C Golgi大量实验的基础上，结合自己的观察，出版了《人和脊椎动物神经系统组织学》一书，确认了神经元的结构^1,2^。      
&emsp;&emsp;一个神经元通常具有多个**树突**，主要用来接受传入信息；而**轴突**只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做**突触**。神经元树突的末端可以接受其他神经或者外部刺激传来的信号，并把信号传给神经元；神经细胞有**兴奋**和**抑制**两种状态，神经细胞的状态取决于其接收到的信号量的大小，当信号量总和超过了某个阈值时，细胞就会激动，产生电信号，并沿着轴突通过突触传递到其他神经元。

[<div align=center><img src=./pictures/sdxx_1.jpg /> </div>](https://www.cnblogs.com/subconscious/p/5058741.html)  
<center>图1 神经元</center>

&emsp;&emsp;受此启发，1943年，心理学家Warren McCulloch和数理逻辑学家Walter Pitts在合作的《A logical calculus of the ideas immanent in nervous activity》^3^ 论文中提出了人工神经网络的概念及神经元的数学模型,即MP模型^4^，从而开启了人工神经网络研究的时代。 

[<div align=center><img src=./pictures/sdxx_2.png /> </div>](http://xiaosheng.me/2017/05/05/article56/)    
<center>图2 神经元模型</center> 

&emsp;&emsp;神经元接收来自 $n$ 个其他神经元传递过来的输入信号 $x$, 这些输入信号通过带权重 $w$ 的连接(connection)进行传递，神经元接收到的总输入值 $\sum_{i=1}^n w_i x_i$ 将与神经元的阈值 $\theta$ 进行比较，然后通过“激活函数”(activation function) $f$ 处理产生神经元的输出 $y=f(\sum_{i=1}^n w_i x_i - \theta)$。常用的激活函数有阶跃函数、Sigmoid函数。   
$$
 sgn(x) = \begin{cases} 1, & \text {x $\geq$ 0}  \\ 0, & \text{x<0} \end{cases}  \text{阶跃函数 1.1}  
$$

$$   
\qquad sigmoid(x) = \frac{1}{1+ e^{-x}}  \text{Sigmoid函数 1.2}
$$  

[<div align=center><img src=./pictures/sdxx_3.png /> </div>](http://xiaosheng.me/2017/05/05/article56/)    
<center>图3 激活函数</center> 

&emsp;&emsp;但是，**MP模型中，权重的值都是预先设置的，因此不能学习**。   

&emsp;&emsp;1949年心理学家Hebb提出了Hebb学习率^5^，认为人脑神经细胞的突触(也就是连接)上的强度是可以变化的，也即权重$w$可以变化。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。    

## 2. 感知机(Perceptron)   
&emsp;&emsp;1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)的Frank Rosenblatt 提出了一个两层的前馈人工神经网络^4,6^，**并给出了相应的学习算法**，从而掀起了人工神经网络的第一次研究热潮。   

### 2.1 感知机模型   

&emsp;&emsp;感知机是一个二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别^7^。
输入 $x \in R^n $表示实例的特征向量，输出 $y \in Y$ 表示实例的类别。输入到输出的映射由如下函数完成：   
$$ f(x)=sign(w \cdot x + b) \tag{2.1}$$   
其中， $w$ 和 $b$ 为感知机模型的参数， $w \in R^n$为权值(weight)或者权值向量(weight vector), $b \in R$ 叫做偏置(bias), $w \cdot x$表示 $w$ 和$b$ 的[内积](https://zh.wikipedia.org/wiki/%E7%82%B9%E7%A7%AF)。 $sign$ 是符号函数， 
  
$$ sign(x) = \begin{cases} +1, & \text {x $\geq$ 0} \\ -1, & \text{x<0} \end{cases} \tag{2.2}$$     

&emsp;&emsp;感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间的所有线性分类模型(linear classfication model)，即函数集合$\lbrace f|f(x)=w \cdot x + b\rbrace $.   
感知机可作如下几何解释：  
线性方程  $w \cdot x + b =0$ 对应于特征空间 $R^n$ 中的一个超平面 $S$,其中$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点(特征向量)分别被分为正、负两类。因此超平面 $S$ 称为分离超平面。 

### 2.2 感知机学习   

#### 2.2.1 数据集的线性可分性   
&emsp;&emsp;给定一个数据集  $T=\lbrace (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \rbrace $,其中， $x_i \in R^n$,$y_i \in \lbrace +1,-1 \rbrace$, $i=1,2, \cdots,N$,如果存在某个超平面$S$   
$$w \cdot x +b =0 \tag{2.3}$$   
能够将数据集的正实例和负实例完全正确的分开，即对所有$y_i=+1$的实例 $i$,有 $w \cdot x +b >0$,对所有 $y_i=-1$的实例$i$，有$w \cdot x +b <0$,则称数据集$T$为线性可分数据集；否则，称数据集$T$线性不可分。   

#### 2.2.2 感知机训练   
&emsp;&emsp;假设训练数据集是线性可分的，为得到可接受的权值向量，我们会从**随机**的权值开始，反复地应用这个感知机到每个训练样例，只要它**误分类样例就修改感知机的权值**。重复这个过程，直到感知机正确分类所有的样例。 换一种方式理解，即调整权值 $w$ 使得模型在所有样本上的输出误差最小，这个误差(或者损失)由输入样本的类别$y$ 和模型实际输出的预测类别 $\hat{y}$  来定义，即需要确定一个函数来衡量输入样本的类别 $y$ 和模型实际输出的预测类别 $\hat{y}$ 间的不同程度。   

&emsp;&emsp; 损失函数的一个自然的选择是误分类点的总数，但是这样的损失函数不是参数$w$,$b$的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面$S$的总距离，这是感知机所采用的的。输入空间$R^n$中任意一点$x_0$到超平面$S$的距离：   
$$\frac{1}{||w||}|w \cdot x_0 + b| \tag{2.4}$$   
其中，$||w||$是$w$的$L_2$  [范数](https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0)      
对于误分类的数据$(x_i,y_i)$来说，   
$$-y_i(w \cdot x_i +b)>0 \tag{2.5}$$   
成立。因为当$w \cdot x_i +b>0$时，$y_i=-1$,而当$w \cdot x_i +b<0$时，$y_i$=+1.因此，误分类点$x_i$到超平面$S$的距离是   
$$-\frac{1}{||w||}y_i(w \cdot x_i +b) \tag{2.6}$$   
这样，假设超平面$S$的误分类点集合为$M$，那么所有误分类点到超平面$S$的总距离为   
$$-\frac{1}{||w||}\sum_{x_i \in M}y_i(w \cdot x_i+b) \tag{2.7}$$   
不考虑$\frac{1}{||w||}$就得到感知机学习的损失函数   
$$L(w,b)=-\sum_{x_i \in M} y_i(w \cdot x_i +b) \tag{2.8}$$

感知机学习的方法就是选择使损失函数最小的参数$w$,$b$. 具体的采用随机梯度下降法(stochastic gradient descent). 首先任意选取一个超平面$w_0$,$b_0$,在第k次迭代中损失函数的梯度为   
$$
\begin{aligned}
\nabla_w L(w_k,b_k) &= -\sum_{x_i \in M} y_i x_i \\
\nabla_b L(w_k,b_k) &= -\sum_{x_i \in M} y_i \\
\end{aligned} 
\tag{2.9}
$$   

随机选取一个误分类点$(x_i,y_i)$更新权值   
$$
\begin{aligned}
w_{k+1} &= w_k + \eta y_i x_i \\
b_{k+1} &= b_k + \eta y_i     \\
\end{aligned}
\tag{2.10}
$$  

上式中$\eta(0<\eta \leq 1)$是步长，又称**学习率(learning rate)**.通过迭代期待损失函数$L(w,b)$不断减小。那么损失函数是否可以减小到0呢？这时候需要考虑感知机算法的收敛性。

#### 2.2.3感知机算法的收敛性   
【收敛定理】：设训练数据集 $T=\lbrace (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \rbrace $是线性可分的，其中， $x_i \in R^n$,$y_i \in \lbrace +1,-1 \rbrace$, $i=1,2, \cdots,N$,则   
(1) 存在满足条件 $||\hat{w}_{opt}||=1$的超平面 $\hat{w}_{opt} \cdot \hat{x}= w_{opt} \cdot x + b_{opt}=0$ 将训练数据集完全正确分开,且存在 $\gamma>0$,对所有 $i=1,2,\cdots,N$   
$$y_i(\hat{w}_{opt} \cdot \hat{x}_i)=y_i(w_{opt} \cdot x_i + b_{opt}) \geq \gamma \tag{2.11}$$   
(2) 令$R= \max_{1 \leq i \leq N}||\hat{x_i}||$,则感知机算法在训练数据集上的误分类次数$k$满足不等式 
$$k \leq \left( \frac{R}{\gamma} \right)^2  \tag{2.12} $$
【证明】:
(1) 由于训练数据集是线性可分的，按照定义，存在超平面可将训练数据集完全正确的分开，取此超平面为$\hat{w}_{opt} \cdot \hat{x}=w_{opt} \cdot x + b_{opt}=0$,使$||\hat{w}_{opt}||=1$.由于对有限的$i=1,2, \cdots ,N$,均有   
$$y_i(\hat{w}_{opt} \cdot \hat{x}_i)=y_i(w_{opt} \cdot x_i + b_{opt})>0 \tag{2.13}$$   
所以存在
$$\gamma= min_{i} \lbrace y_i(w_{opt} \cdot x_i + b_{opt}) \rbrace \tag{2.14}$$   
使   
$$y_i(\hat{w}_{opt} \cdot x_i + b_{opt}) \geq \gamma$$   

(2) 感知机算法从$\hat{w}_0=0$开始，如果实例被误分类，则更新权重。令$\hat{w}_{k-1}$是第k个误分类实例之前的扩充权重向量，即   
$$\hat{w}_{k-1} = (w_{k-1}^{T},b_{k-1})^T \tag{2.15}$$   
则第k个误分类实例的条件是   
$$y_i(\hat{w}_{k-1} \cdot \hat{x}_i)=y_i(w_{k-1} \cdot x_i + b_{k-1}) \leq 0 \tag{2.16}$$   
若 $(x_i,y_i)$是被$\hat{w}_{k-1}=(w_{k-1}^{T},b_{k-1})^{T}$误分类的数据，则$w$和$b$的更新是
$$
\begin{aligned}
w_k & \longleftarrow w_{k-1} + \eta y_i x_i \\
b_k & \longleftarrow b_{k-1} + \eta y_i \\
\end{aligned} \tag{2.17}
$$    
即
$$\hat{w}_k=\hat{w}_{k-1} + \eta y_i \hat{x}_i \tag{2.18}$$      
下面推导两个不等式:
(i)   
$$\hat{w}_k \cdot \hat{w}_{opt} \geq k \eta \gamma  \tag{2.19}$$   
由式(2.18)及式(2.11)得   
$$
\begin{aligned}
\hat{w}_{k} \cdot \hat{w}_{opt} &=  \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta y_i \hat{w}_{opt} \cdot \hat{x}_i \\
 & \geq \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma \\
 \end{aligned}
$$   
由此递推即得不等式(2.19)      
$$\hat{w}_{k} \cdot \hat{w}_{opt} \geq \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma  \geq \hat{w}_{k-2} \cdot \hat{w}_{opt} + 2 \eta \gamma  \geq \cdots \geq k \eta \gamma$$   

(ii)   
$$||\hat{w}_k||^2 \leq k \eta^2 R^2   \tag{2.20}$$   
由式(2.19)及式(2.16)得   
$$
\begin{aligned}
||\hat{w}_k||^2 &= ||\hat{w}_{k-1}||^2 + 2 \eta y_i \hat{w}_{k-1} \cdot \hat{x}_i + \eta^2 ||\hat{x}_i||^2  \\
&\leq ||\hat{w}_{k-1}||^2  +  \eta^2 ||\hat{x}_i||^2 \\
&\leq ||\hat{w}_{k-1}||^2 + \eta^2 R^2 \\
&\leq ||\hat{w}_{k-2}||^2 + 2 \eta^2 R^2  \leq \cdots \\
&\leq k \eta^2 R^2
\end{aligned}
$$   
结合不等式(2.19)及式(2.20)即得   
$$
\begin{aligned}
k \eta \gamma &\leq \hat{w}_k \cdot \hat{w}_{opt} \leq ||\hat{w}_k|| ||\hat{w}_{opt}|| \leq \sqrt{k} \eta R  \\
k^2 \gamma^2 &\leq k R^2
\end{aligned}
$$   
于是   $k \leq \left( \frac{R}{\gamma} \right)^2$


&emsp;&emsp;定理表明，误分类的次数$k$是有上界的，经过有限次搜索可以找到将训练数据集完全正确分开的分离超平面。也即训练数据集线性可分时，感知机学习算法是收敛的。但是算法的解并不唯一，这依赖于权值初始值的选择和误分类点的选择顺序。为了得到唯一的超平面，需要对超平面增加约束条件,这就引申出 **支持向量机(Support Vector Machine,SVM)** 的构思。

#### 2.2.4感知机算法的局限性   
&emsp;&emsp; 在[2.2.3 感知机算法的收敛性](#223感知机算法的收敛性) 中我们证明了感知机在**线性可分数据集**上是收敛的，那么在线性不可分数据集上的表现如何呢？1969年，Marvin Minsky和Seymour Papert在《Perceptrons》书中，仔细分析了以感知机为代表的单层神经网络系统的功能及局限，**证明感知机不能解决简单的异或(XOR)等线性不可分问题**。由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究第一次跌落到谷底。   
考虑如下的四元数据集 $T_0 = \lbrace (0,0),(0,1),(1,0),(1,1) \rbrace$,示意图如下：

可以看到这是一个线性不可分数据集，无法找到一个平面将数据集完全正确的分开。   


## 3 两层神经网络   
&emsp;&emsp; 在[2.2.4感知机算法的局限性](#224感知机算法的局限性)中介绍过单层神经网络，也即感知机无法解决异或(XOR)问题，在Marvin Minsky发表这一结论的时候，研究人员已经意识到增加一个计算层，组成两层神经网络可以解决异或问题，但是当时并没有有效的算法来训练这样的网络，直到10年以后，**反向传播(Backpropagation,BP)算法**的提出，才解决了两层神经网络的计算问题，由此带来了神经网络的第二次研究热潮。

下图是一个典型的两层神经网络的示意图：     
[<div align=center><img src=./pictures/sdxx_4.png /> </div>](http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)    
<center>图4 两层神经网络</center> 

&emsp;&emsp; Layer L1为输入层；中间所有节点组成的一层叫做隐藏层L2，因为我们不能在训练样本集中观测到它们的值；L3为输出层。标上 "+1" 的圆圈表示偏置(bias)。以上神经网络的例子中有3个输入单元(偏置单元不计在内)，3个隐藏单元及一个输出单元.    
我们用 $n_l$ 来表示网络的层数，本例中$n_l=3$ ，我们将第 $l$ 层记为 $L_1$ ，于是 $ L_1$ 是输入层，输出层是 $L_{n_l}$ 。本例神经网络有参数 $(W,b) = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})$，其中 $W^{(l)}_{ij}$ 是第 $l$ 层第 $j$ 单元与第 $l+1$ 层第 $i$ 单元之间的联接参数, $b^{(l)}_i$ 是第$l+1$ 层第 $i$ 单元的偏置项。因此在本例中，$W^{(1)} \in \Re^{3\times 3}$,$W^{(2)} \in \Re^{1\times 3}$。同时，我们用 $s_l$ 表示第$l$ 层的节点数(偏置单元不计在内)。

需要说明的是，网络中的参数$(W,b)$的初始值都是**随机初始化**的。   

### 3.1 前向传播   
&emsp;&emsp; 我们用 $a_i^{(l)}$ 表示第 $l$层第$i$单元的激活值(输出值)。当 $l=1$ 时， $a^{(1)}_i = x_i$ ，也就是第 $i$ 个输入值(输入值的第 $i$ 个特征)。对于给定参数集合 $W,b$ ，我们的神经网络就可以按照函数 $h_{W,b}(x)$ 来计算输出结果。    
$$
\begin{aligned}
a_1^{(2)} &= f(W_{11}^{(1)}x_1+ W_{12}^{(1)}x_2 + W_{13}^{(1)}x_3 + b_1^{(1)}) \\
a_2^{(2)} &= f(W_{21}^{(1)}x_1+ W_{22}^{(1)}x_2 + W_{23}^{(1)}x_3 + b_2^{(1)}) \\
a_3^{(2)} &= f(W_{31}^{(1)}x_1+ W_{32}^{(1)}x_2 + W_{33}^{(1)}x_3 + b_3^{(1)}) \\
h_{w,b}(x) &= a_1^{(3)} = f(W_{11}^{(2)}a_1^{(2)} + W_{12}^{(2)}a_2^{(2)} + W_{13}^{(2)}a_3^{(2)} + b_1^{(2)})
\end{aligned}
$$    

用 $ z^{(l)}_i$ 表示第 $l$ 层第 $i$ 单元输入加权和(包括偏置单元)，比如， $z_i^{(2)} = \sum_{j=1}^n W^{(1)}_{ij} x_j + b^{(1)}_i$ ，则 $a^{(l)}_i = f(z^{(l)}_i)$。上面的公式向量化表述为：   
$$
\begin{aligned}
z^{(2)} &= W^{1}x + b^{1} \\
a^{(2)} &= f(z^{(2)}) \\
z^{(3)} &= W^{(2)} a^{(2)} + b^{(2)} \\
h_{w,b}(x) &= a^{(3)} = f(z^{(3)})
\end{aligned}
$$   
上述过程称为**前向传播**，即输入值经过每一个网络层向前传输，直到最后的输出层。   

### 3.2 反向传播   
&emsp;&emsp; 对于给定的样本集， $\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}$，它包含 $m$ 个样例。我们可以用批量梯度下降法来求解神经网络。具体来讲，对于单个样例$(x,y)$，其代价函数为   
$$
\begin{aligned}
J(W,b; x,y) = \frac{1}{2} \left\| h_{W,b}(x) - y \right\|^2
\end{aligned}
$$   
给定一个包含 $m$ 个样例的数据集，我们可以定义整体代价函数为：   
$$
\begin{aligned}
J(W,b)
&= \left[ \frac{1}{m} \sum_{i=1}^m J(W,b;x^{(i)},y^{(i)}) \right]
                       + \frac{\lambda}{2} \sum_{l=1}^{n_l-1} \; \sum_{i=1}^{s_l} \; \sum_{j=1}^{s_{l+1}} \left( W^{(l)}_{ji} \right)^2
 \\
&= \left[ \frac{1}{m} \sum_{i=1}^m \left( \frac{1}{2} \left\| h_{W,b}(x^{(i)}) - y^{(i)} \right\|^2 \right) \right]
                       + \frac{\lambda}{2} \sum_{l=1}^{n_l-1} \; \sum_{i=1}^{s_l} \; \sum_{j=1}^{s_{l+1}} \left( W^{(l)}_{ji} \right)^2
\end{aligned}
$$
以上关于$J(W,b)$定义中的第一项是一个均方差项。第二项是一个规则化项(也叫**权重衰减项**)，其目的是减小权重的幅度，防止过度拟合。 权重衰减参数$\lambda$用于控制公式中两项的相对重要性。    

我们的目标是针对参数 $W$ 和 $b$ 来求其函数 $J(W,b)$ 的最小值,这里还是采用**梯度下降法**，对损失函数 $J(W,b)$求关于 $W$ 和 $b$的偏导，然后更新 $W$ 和 $b$，   
$$
\begin{aligned}
W_{ij}^{(l)} &= W_{ij}^{(l)} - \alpha \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b) \\
b_{i}^{(l)} &= b_{i}^{(l)} - \alpha \frac{\partial}{\partial b_{i}^{(l)}} J(W,b)
\end{aligned}
$$
其中 $\alpha$ 是**学习率**。   
$$
\begin{aligned}
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b) &=
\left[ \frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x^{(i)}, y^{(i)}) \right] + \lambda W_{ij}^{(l)} \\
\frac{\partial}{\partial b_{i}^{(l)}} J(W,b) &=
\frac{1}{m}\sum_{i=1}^m \frac{\partial}{\partial b_{i}^{(l)}} J(W,b; x^{(i)}, y^{(i)})
\end{aligned}
$$

到目前为止，我们还没有具体指明的激活函数，在早期的两层神经网络中，较为常用的是Sigmoid函数，即$f(x)=\frac{1}{1+e^{-x}}$,这个函数的一阶导数   
$$
\begin{aligned}
f^\prime(x) &= \frac{e^{-x}}{(1+e^{-1})^2}  \\
            &= \frac{1}{1+e^{-x}} - \frac{1}{(1+e^{-x})^2}  \\
            &= \frac{1}{1+e^{-x}}(1- \frac{1}{1+e^{-x}})    \\
            &= f(x)(1-f(x))
\end{aligned}
\tag{3.3}
$$     
将其带入$W,b$的偏导公式中，以第一层和第二层中的参数$W$为例，其偏导分别如下：   
$W_{11}^{(1)}$的偏导      
$$
\begin{aligned}
\frac{\partial}{\partial W_{11}^{(1)}} J(W,b) &= \left[ \frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial W_{11}^{(1)}}  \frac{1}{2}{||h_{w,b}(x^{(i)}) - y^{(i)}||}^2 \right] + \lambda W_{11}^{(1)}  \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})\frac{\partial}{\partial W_{11}^{(1)}}  f(z^{(3)}) \right] + \lambda W_{11}^{(1)}   \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})f(z^{(3)})(1-f(z^{(3)}))\frac{\partial}{\partial W_{11}^{(1)}}  z^{(3)} \right] + \lambda W_{11}^{(1)}   \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})f(z^{(3)})(1-f(z^{(3)})) W^{(2)} \frac{\partial}{\partial W_{11}^{(1)}}  a^{(2)} \right] + \lambda W_{11}^{(1)}   \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})f(z^{(3)})(1-f(z^{(3)})) W^{(2)} f(z^{(2)})(1-f(z^{(2)}))\frac{\partial}{\partial W_{11}^{(1)}}  z^{(2)} \right] + \lambda W_{11}^{(1)}   \\ 
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})f(z^{(3)})(1-f(z^{(3)})) W^{(2)} f(z^{(2)})(1-f(z^{(2)}))x_1 \right] + \lambda W_{11}^{(1)}   \\
\end{aligned}
$$   


$W_{11}^{(2)}$的偏导   
$$
\begin{aligned}
\frac{\partial}{\partial W_{11}^{(2)}} J(W,b) &= \left[ \frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial W_{11}^{(2)}}  \frac{1}{2}{||h_{w,b}(x^{(i)}) - y^{(i)}||}^2 \right] + \lambda W_{11}^{(2)}  \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})\frac{\partial}{\partial W_{11}^{(2)}}  f(z^{(3)}) \right] + \lambda W_{11}^{(2)}   \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})f(z^{(3)})(1-f(z^{(3)}))\frac{\partial}{\partial W_{11}^{(2)}}  z^{(3)} \right] + \lambda W_{11}^{(2)}   \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})f(z^{(3)})(1-f(z^{(3)})) a_1^{(2)}  \right] + \lambda W_{11}^{(2)}   \\
&= \left[ \frac{1}{m} \sum_{i=1}^m (f(z^{(3)})-y^{(i)})f(z^{(3)})(1-f(z^{(3)})) f(z^{(2)})  \right] + \lambda W_{11}^{(2)}   \\
\end{aligned}
$$

&emsp;&emsp; 注意到第一层的参数的偏导比第二层的参数的偏导多了$W^{(2)} (1-f(z^{(2)})) x_1$，参数 $W$ 以及Sigmod 函数的取值都是0到1间的小数，所以当网络层数加大时，越靠近输入层的参数的梯度值会越来越小，出现**梯度消失问题**，导致低层参数难以更新，初始值对网络影响较大。   


### 3.3 多层神经网络的困境   
&emsp;&emsp; 反向传播算法为神经网络的实际运用找到了一条切实可行的路径，并且研究人员证明了两层的神经网络能够逼近任意的非线性函数，加大神经网络层数，可以提高网络的表达能力。但是加大神经网络层数的同时，也遇到了一些问题。
- 隐含层的神经元的个数的选取
- 参数更新时，学习率的选择难以确定，学习率过高，网络难以收敛，学习率过低，网络学习太慢
- 网络层数加大时，出现**梯度消失问题**，网络难以调整
- 参数的初始值对网络的影响较大

&emsp;&emsp;与此同时，Vapnik等人发明的SVM(Support Vector Machines，支持向量机)算法，其理论完备，无需调参，使用简便，迅速赢得了研究者们的青睐，神经网络的研究又一次陷入低潮。   


## 4 卷积神经网络(CNN)     
&emsp;&emsp; 在两层神经网络模型提出后不久，20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而提出了卷积神经网络(Convolutional Neural Networks,CNN)。卷积神经网络的结构与神经网络稍有不同，**卷积神经网络的输入是矩阵**，而神经网络的输入是向量；但是训练方式都是采用反向传播算法；卷积神经网络由输入层、卷积层、激活函数、池化层、全连接层组成，介绍卷积神经网络之前，我们先看下卷积神经网络中的两个基本操作，**卷积**和**池化**。      

### 4.1 卷积      
[<div align=center><img src=./pictures/sdxx_5.gif /> </div>](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)    
<center>图5 卷积</center>    

&emsp;&emsp; 给定一张矩阵(上图左侧的Image)，用卷积核(上图左侧Image中移动的黄色小矩阵)从矩阵的左上角开始，从左到右，从上到下扫描矩阵；在每个扫描到的小矩阵中的元素与卷积核的对应位置上的值做乘法，然后所有值相加输入到激活函数中，得到卷积后该位置的值；最终得到Convolved Feature(也称 Feature Map);这里的卷积核中的值类似神经网络中的权值，也是需要随机初始化，并通过算法训练调整。卷积核每次移动的距离称为**步长** 。 

### 4.2 池化      
[<div align=center><img src=./pictures/sdxx_6.png /> </div>](https://blog.csdn.net/huahuazhu/article/details/73469491)    
<center>图6 池化</center> 

&emsp;&emsp; 池化有最大池化和平均池化以及随机池化等，就是用一个 $m \times n$ 的矩阵从左到右，从上到下扫描特征矩阵，在每个扫描到的小矩阵内，按一定规则选取出一个值。   

### 4.3 LeNet-5卷积神经网络结构     
[<div align=center><img src=./pictures/sdxx_7.png /> </div>](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)    
<center>图7 手写数字识别神经网络</center>    

&emsp;&emsp; 上图是用于[手写数字识别](http://yann.lecun.com/exdb/mnist/)的LeNet-5卷积神经网络。输入是由 32 x 32 个像素点构成的图片，网络中包含三个卷积层，两个采样层。      
&emsp;&emsp; C1为卷积层，包含6个feature maps,由6个 5 x 5的卷积核与输入层做卷积得到；C1层包含`(5*5 +1)*6=156`个参数，含`6*28*28*(5*5+1)=122304`个连接；S2为采样层，由C1层做池化得到，池化窗口为 2 x 2，每个窗口中的元素相加，乘以一个系数，再加上偏置，输入到sigmod函数，所以这一层有`(1+1)*6=12`个参数,`6*14*14*5=5880`个连接；C3为卷积层，含16个feature maps,这16个feature maps和S2层的feature maps的连接方式如下表：  
$$  
\begin{array}{c|lcr}
 & \text{0} & \text{1} & \text{2} & \text{3} & \text{4} & \text{5} & \text{6} & \text{7} & \text{8} & \text{9} & \text{10} & \text{11} & \text{12} & \text{13} & \text{14} & \text{15}\\
\hline
0  & X &   &   &   & X & X & X &  &   & X & X & X & X &  & X & X &\\
1  & X & X &   &   &   & X & X & X &  &  & X & X & X & X &  & X & \\
2  & X & X & X &   &   &   & X & X & X &  &  & X &  & X & X & X & \\
3  &   & X & X & X &   &   & X & X & X & X &  &   & X &  & X & X & \\
4  &   &   & X & X & X &   &   & X & X & X & X &  & X & X &  & X & \\
5  &   &   &   & X & X & X &   &  & X & X & X & X &   & X & X & X &\\
\end{array} 
$$   

&emsp;&emsp; 上表表示C3层的第一个map由S2层第1,2,3个map得到；C3层的第2个map由S2层第2,3,4个map得到；依次类推；C3层有`(5*5*3+1)*6+(5*5*4+1)*9+(5*5*6+1)*1=1516`个参数，`(5*5*3+1)*6*100+(5*5*4+1)*9*100+(5*5*6+1)*1*100=151600`个连接；S4为采样层，由C3采样得到，S4有`(1+1)*16=32`个参数，`16*5*5*5=2000`个连接；C5是卷积层，含120个feature maps，由`120*16`个`5*5`的卷积核与S4做卷积得到，注意这里C5层的每个feature map是由16个`5*5`的卷积核做卷积得到的，所以这一层共有 `120*(16*5*5+1)=48120`个参数；F6为全连接层，与C5全连接得到，含`(120+1)*84=10164`个参数;整个网络共有`156+12+1516+32+48120+10164=60000`个参数，`122304+5880+151600+2000+48120+10164=340068`个连接。

&emsp;&emsp; 卷积神经网络的一个重要特征就是**权值共享**，也就是每个feature map都共享卷积核的参数，使得参数的数量急速下降，相比于全连接网络大大降低了网络的训练难度；如上的LeNet-5网络的参数约为连接数的$1/6$。   




## 5 深度信念网络(DBN)   
&emsp;&emsp; 2006年，Hinton在Science上发表了[A fast learning algorithm for deep belief nets](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)一文，提出了**深度信念网络（Deep Belief Nets,DBN）**，并用Deep Learning来命名这一网络，以区别于原来的神经网络，从而开启了深度学习的第三次研究热潮。DBN网络中采用**逐层预训练**的方式解决了前面提到的多层神经网络低层参数难以更新的问题。为了更好的理解DBN网络，我们先介绍自编码器和波尔兹曼机。   


### 5.1 自编码器   
&emsp;&emsp;1986年Rumelhart和Hinton在[ Learning representations by back-propagating errors](http://lia.disi.unibo.it/Courses/SistInt/articoli/nnet1.pdf)中提出了自编码网络；自编码器是一个两层的**无监督学习**神经网络，包含编码(encode)和解码(decode)两个步骤;类比图像压缩算法，编码即将输入数据转换为另一种占用空间更小的数据，解码即将编码后的数据再反转为原始的输入数据；自编码器是一个有损压缩算法。   
[<div align=center><img src=http://ufldl.stanford.edu/wiki/images/thumb/f/f9/Autoencoder636.png/400px-Autoencoder636.png /> </div>](http://ufldl.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7)    
<center>图8 自编码网络</center>    

&emsp;&emsp; L1是输入层，L2是隐含层，L3是输出层；对于一个无类别标签的样本$ \textstyle \{x^{(1)}, x^{(2)}, x^{(3)}, \ldots\} ，其中 \textstyle x^{(i)} \in \Re^{n}$,网络的目的是让输出尽可能接近输入，即 $h_{w,b}(x) \approx x$，令网络中的隐含层的神经元数目为m，当网络稳定后，则对于任意一个样本，输入到网络中，都可以用m维的向量来表示这个样本，即实现数据压缩。   
&emsp;&emsp; 通常为了求解的方便，会对网络加以各种限制，得到**稀疏自编码网络**、**栈式自编码网络**等各种自编码网络。   


### 5.2 受限玻尔兹曼机    
&emsp;&emsp; 

### 5.3 深度信念网络   

## 6 AlexNet CNN      
&emsp;&emsp;2012年，Hinton与其学生Alex Krizhevsky,Ilya Sutskever发表了[ImageNet Classification with Deep Convolutional Neural Networks](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf)一文，提出了AlexNet(也称ImageNet CNN)网络，并利用这一网络获得了当年[ImageNet](http://www.image-net.org/)图像竞赛的第一名，AlexNet网络将top-5的错误率降低至了16.4%，相比第二名的成绩26.2%错误率有了巨大的提升。这一结果激发了工业界极大的热情，开始真正重视深度学习，并投入巨资推动其发展。   
&emsp;&emsp;AlexNet网络首次采用了GPU加速，并且采用了 ReLu 非线性激活函数、Dropout、Local Response Normalization、Overlapping Pooling、Data Augmentation等后续网络广为采用的技术。

### 6.1 ReLu 非线性激活函数     
&emsp;&emsp;AlexNet网络中采用的是ReLu(Rectified Linear Unit)激活函数； 

$$
ReLu(x)=f(x)=max(0,x) \\
\qquad ReLu^\prime(x) =f^\prime(x)= \begin{cases} 1, & \text {x \geq 0}  \\ 0, & \text{x<0} \end{cases}  \tag{6.1}
$$     

&emsp;&emsp;这个激活函数带来两个改变，一是求导简单，加速了网络的训练，二是导函数的值取值$\{0,1\}$,解决了Sigmoid在网络较深时的梯度消失问题。

### 6.1 Dropout        
&emsp;&emsp;Dropout 是Hinton在[Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf)中提出来的，是一种防止过拟合的技术；是指在网络训练过程中，随机的让网络中的部分隐含层的节点不工作，网络前向传播的时候，选取的这部分节点的值不向前传播，梯度反向传播的时候也不更新其权值。需要说明的是，原文章中是用大量实验证明Dropout可以防止过拟合，并没有理论上的证明。   

### 6.2 局部响应正则化   
&emsp;&emsp;根据原文[ImageNet Classification with Deep Convolutional Neural Networks](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf)解释，局部响应正则化(Local Response Normalization,LRN)是对神经元的输出做正则化，   
$$b_{x,y}^i = a_{x,y}^i / \left(k + \alpha \sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)} (a_{x,y}^j)^2 \right)^{\beta} \tag{6.2}$$    
$a_{x,y}^i$表示在活跃神经元$(x,y)$位置上用卷积核$i$做卷积然后输入到ReLu函数得到的结果，$b_{x,y}^i$是对应位置的正则化响应。局部响应正则增强了神经元间的竞争，增强了模型的泛化能力。      

<!-- &emsp;&emsp;在局部响应正则提出后，研究者后续又提出了 [Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf)、[Layer Norm](https://arxiv.org/pdf/1607.06450.pdf)、[Weight Norm](https://arxiv.org/pdf/1602.07868.pdf)、[Cosine Norm](https://arxiv.org/pdf/1702.05870.pdf) 等正则化方法。-->

### 6.3  Data Augmentation  
&emsp;&emsp;原文中是用了两种形式的Data Augmentation，一是在原始输入的`256*256`的图像中随机的截取`224*224`大小的区域(以及水平翻转的镜像),使得样本量增加了2048倍；二是对训练集做PCA得到图像RGB值的主要特征，然后对高斯扰动,即对于每个RBG图像$I_{xy}= [I_{xy}^R,I_{xy}^G,I_{xy}^B]^T $，加上$[p_1,p_2,p_3][\alpha_1 \lambda_1,\alpha_2 \lambda_2,\alpha_3 \lambda_3]^T$ ,$p_i,\lambda_i$是特征向量和特征值，$\alpha_i$是高斯随机变量。   
&emsp;&emsp;Data Augmentation 大大减轻了网络过拟合的风险。

### 6.4  AlexNet   
<div align=center><img src=./pictures/sdxx_9.png /> </div> 
<center>图9 AlexNet</center>  

&emsp;&emsp;AlexNet网络前面5层是卷积层，后面三层是全连接层，最终softmax输出是1000类，作者在实验中发现，移除任意一层都会降低网络最终的效果。
输入图片为`224*224*3`,经过一个预处理变为`224*224*3`,第一个卷积层含96个`11*11*3`卷积核,步长为4，输出为`55*55*96`($\lfloor 227-11 \rfloor /4+1=55,\lfloor \rfloor$表示向下取整,卷积时，用$\lfloor img\_size - filter\_size \rfloor /stride +1 = new\_feture\_size$,这个公式来计算feature  map的大小)，96个feature map分为2组，输入到两块GPU上；   
第二个卷积层含256个`5*5*48`卷积核，第三个卷积层有384个`3*3*256`卷积核，第四个卷积层有384个`3*3*192`卷积核,第五个卷积层有256个`3*3*192`卷积核，全连接层有4096个神经元；


## 7 递归神经网络(RNN)   

## 8 残差神经网络(ResNet)     
Highway Networks & Deep Residual Networks & lowrank-highwaynetwork

## 9 对抗神经网络(WGAN)   
### 9.1 GAN
### 9.2 WGAN


## 10 CapsNet   


Reference:
1. [百度百科：神经元](https://baike.baidu.com/item/%E7%A5%9E%E7%BB%8F%E5%85%83/674777)
2. [维基百科：神经元](https://zh.wikipedia.org/wiki/%E7%A5%9E%E7%B6%93%E5%85%83)   
3. [McCulloch W S, Pitts W. A logical calculus of the ideas immanent in nervous activity[J]. The bulletin of mathematical biophysics, 1943, 5(4): 115-133.](http://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)
4. [维基百科：感知机](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8)   
5. [维基百科：赫布理论](https://zh.wikipedia.org/wiki/%E8%B5%AB%E5%B8%83%E7%90%86%E8%AE%BA)   
6. [Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain[J]. Psychological review, 1958, 65(6): 386.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)      
7. [李航. 统计学习方法[J]. 清华大学出版社, 北京, 2012.](https://github.com/yuanliangding/books/blob/master/%E8%AE%A1%E7%AE%97%E6%9C%BA%E2%97%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E2%97%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95(%E6%9D%8E%E8%88%AA).pdf)   

